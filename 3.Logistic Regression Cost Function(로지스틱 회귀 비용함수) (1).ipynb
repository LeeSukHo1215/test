{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3.Logistic Regression Cost Function(로지스틱 회귀 비용함수).ipynb","provenance":[],"toc_visible":true,"collapsed_sections":[],"authorship_tag":"ABX9TyPSsklz7L5XJcRhmU6YdgoT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 3.Logistic Regression Cost Function(로지스틱 회귀 비용함수)"],"metadata":{"id":"VXRxCpOOHXcl"}},{"cell_type":"markdown","source":["## 정의\n","* 원래의 값과 가장 오차가 작은 가설함수 를 도출하기 위해 사용되는 함수\n"],"metadata":{"id":"_qoI5JnWHZ9X"}},{"cell_type":"markdown","source":["비용 함수란\n","\n","훈련 세트 전체에 대해 얼마나 잘 추측되었는지 측정해주는 함수입니다\n","\n","$$J(w,b) =\\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y^{i}},y^{i}) = -\\frac{1}{m}\\sum_{i=1}^{m}[ylog\\hat{y} + (1-y)log(1- \\hat{y})]$$\n","\n","비용 함수 J는 파라미터 w와 b에 대해\n","\n","손실 함수를 각각의 훈련 샘플에 적용한 값의\n","\n","합들의 평균 즉 m으로 나눈 값"],"metadata":{"id":"LUw6Cd4OHb2P"}},{"cell_type":"markdown","source":["##설명 \n","로지스틱 회귀 모델의 파라미터 w와 b를\n","주어진 m개의 훈련 샘플로 학습할 때\n","훈련 세트를 바탕으로 출력한 y^(i)의 예측값이\n","참값 y^(i)에 가까워지도록 하려고합니다.\n","\n","$${(x^{1}, y^{1}),\\ (x^{2}, y^{2}),\\ … \\ (x^{m}, y^{m})}$$  $$\\hat{y}^{(i)}\\approx y^{(i)}$$\n","\n","이 공식은 훈련 샘플 x가 주어졌을 때\n","y의 예측값을 정의합니다."],"metadata":{"id":"PRM47v2eHgZf"}},{"cell_type":"markdown","source":["### 틀린예시\n","알고리즘이 출력한 y의 예측값과 참 값 y의\n","\n","제곱 오차의 반으로 손실 함수를 정의할 수 있습니다\n","\n","$$L(\\hat{y},y) = \\frac{1}{2}(\\hat{y}- y)^{2}$$ \n","이런 방식을 쓸 수 있지만\n","\n","로지스틱 회귀에서는 주로 사용하지 않습니다\n"],"metadata":{"id":"jHWQTCU0HrEv"}},{"cell_type":"markdown","source":["#### 제곱방식 사용하지 않는 이유\n","\n","파라미터들을 학습하기 위해 풀어야할 최적화 함수가 볼록하지 않기 때문입니다\n","그러므로 여러 개의 지역 최적값을 가지고 있게 되어 문제가 생깁니다\n","\n","\n","\n","* \n","$$ y =1$$\n","\n","손실 함수 L(y의 예측값, y)는 그냥 −log(y의 예측값)이 됩니다\n","\n","$$ L(\\hat{y},y)=-log\\hat{y}$$\n","\n","\n","\n","\n","−log(y의 예측값)이 최대한 커지기를 원할 것입니다\n","\n","그러려면 log(y의 예측값)이 최대한 커져야 할 것이며\n","\n","따라서 y의 예측값이 최대한 커야 합니다\n","\n","하지만 y의 예측값은 시그모이드 함수 값이기 때문에 1보다 클 수 없습니다\n","\n","즉 y의 예측값이 1보다 클 순 없으므로 1에 수렴하길 원한다는 뜻입니다\n","\n","\n"," * \n","$$ y =0$$ \n","손실 함수는 −log(1−y의 예측값)이 됩니다\n","$$ L(\\hat{y},y)=-log(1- \\hat{y})$$\n","\n","따라서 손실 함숫값을 줄이고 싶다면\n","\n","log(1−y의 예측값)이 최대한 커야 합니다\n","\n","즉 y의 예측값이 최대한 작아야 한다는 것을 알 수 있습니다\n","\n","y의 예측값은\n","\n","0과 1사이어야 하므로 y가 0이면 손실 함수는\n","\n","y의 예측값이 0에 수렴하도록 파라미터들을 조정할 것입니다\n","\n","\n","즉\n"," *  y가 1일 때 y의 예측값이 크고 y가 0일 때 y의 예측값이 작은성질을 가지고 있는 함수들은 많기때문에 제곱오차를 쓰지 않는다.\n"],"metadata":{"id":"bM0CnwjiHuNX"}},{"cell_type":"markdown","source":["### 맞는 예시\n","로지스틱 회귀에서는 이러한 손실 함수를 대신 씁니다\n","\n","$$L(\\hat{y},y) = -(ylog\\hat{y} + (1-y)log(1- \\hat{y}))$$\n","\n"],"metadata":{"id":"0HLYA1TcHzyu"}},{"cell_type":"markdown","source":["##결론 \n","* 손실 함수가 하나의 훈련 샘플에 적용이 된다\n","\n","* 비용 함수는 파라미터의 비용처럼 작용한다  \n","\n","\n","즉 로지스틱 회귀 모델을 학습하는 것이란\n","\n","손실 함수 J를 최소화해주는 파라미터들 w와 b를 찾는 것입니다"],"metadata":{"id":"FY_RSdK3H1hn"}},{"cell_type":"markdown","source":["### 참조\n","https://www.youtube.com/watch?v=SHEPb1JHw5o&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=9"],"metadata":{"id":"2jb52g_781oR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jLvyR40mGm3b"},"outputs":[],"source":[""]}]}